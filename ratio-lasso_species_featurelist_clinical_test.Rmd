---
title: "Ratio Lasso - Clicical Species Test Set Prediction"
subtitle: "Renamed Sample Files"
author: 
  - "Code author: Jacob Mardick"
  - "Code run by: `r Sys.info()[['user']]`"
date: "`r format(Sys.time(), '%B %d, %Y %H:%M')`"
knit: (function(inputFile, encoding) { 
      proj_name <- tools::file_path_sans_ext(basename(inputFile));
      out_dir <- file.path("outputs", paste0(proj_name, "_", Sys.Date()));
      if(!file.exists(out_dir)) {   dir.create(out_dir) };
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=file.path(dirname(inputFile), 
                        out_dir, 
                        paste0(format(Sys.time(), "%Y-%m-%d_%H.%M"),"_", proj_name, ".html"))) 
                        })

output: 
  html_document:
    keep_md: yes
    df_print: paged
    toc: false
geometry: margin=0.5in
editor_options: 
  markdown: 
    wrap: 72
---

```{=html}
<style type="text/css">
.main-container {
max-width: 1600px;
margin-left: auto;
margin-right: auto;
}
</style>
```

```{css, echo=FALSE}
h1,h2, h3, h4, h5, p {
text-align: center;
font-size: 20px;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE, cache = FALSE, results = "hold")
```

```{r libraries, message = FALSE, warning = FALSE}
library(rmarkdown)
library(knitr)
library(kableExtra)
library(ggpubr)

library(tidyverse)
library(reshape2)
library(readxl)
library(rawrr)
library(sqldf)

library(stringr)
library(doParallel)
library(fcluster)
library(johnfuncs)

library(caret) ## automates supervised learning (predictive modeling)
library(glmnet) ## for training, cross validation, and testing model
library(pROC) ## for plotting ROC curve
library(scales) ## for integer y-axis on histogram
```

```{r user input}
## processing settings and model file
model_path <- gsub("\\\\", "/", r"(C:\Users\Jacob\OneDrive - Baylor College of Medicine\Documents\Manoj\Microbial-Project\outputs\ratio-lasso_species_featurelist_2025-11-12\2025-11-12_16.39_ratio-lasso_species_featurelist_files\2025-11-12_16.41_ratio-lasso_species_featurelist_model.RData)")

## Full path to folder with sample files (excel or csv, or raw Thermo files)
sample_dir <- gsub("\\\\", "/", r"(C:\Users\Jacob\Box\Eberlin_Lab_BCM\People\MANOJ\Renamed_Gram_positive_Gram_negative_Isolates_May2025\Highly Infected sample\Extracted Data excel sheet)")

## Full path to feature list, otherwise NULL
feature_file <- gsub("\\\\", "/", r"(C:\Users\Jacob\Box\Eberlin_Lab_BCM\Projects\Bacterial Identification\feature_peak_lists\Master Feature List July_2025_dedup-formulacorrected-20251111.csv)")
```

```{r create directory for output files, include = FALSE}
proj_name <- tools::file_path_sans_ext(basename(rstudioapi::getSourceEditorContext()$path))

out_dir <- file.path("outputs", paste0(proj_name, "_", Sys.Date()))

if(!file.exists(out_dir)) {   
  dir.create(out_dir, recursive = TRUE) 
  }

files_dir <- file.path(out_dir, paste0(format(Sys.time(), "%Y-%m-%d_%H.%M"), "_", proj_name, "_files"))

if(!file.exists(files_dir)) {   
  dir.create(files_dir, recursive = TRUE) 
  }
```

```{r classes sample_file_ext and fixed_objects}
load(model_path)

## file extension
sample_file_ext <- "xlsx"

## fixed objects for exporting to parallel computing
fixed_objects <- list(mass_range = mass_range)

classes_path <- gsub(file.path(sample_dir, "/"), "", list.dirs(sample_dir)[c(-1)], fixed=TRUE)
tmp <- strsplit(classes_path, "/")
tmp <- tmp[lapply(tmp,length)==2]
classes_path <- lapply(tmp, function(x) paste(x, collapse = '/'))

## match classes_path and final classes
classes_path <- classes_path[str_detect(classes_path, str_c(classes, collapse ="|"))]

## Missing classes added for Lasso
present_classes <- unique(basename(unlist(classes_path)))
missing_classes <- setdiff(classes, present_classes)
missing_paths <- file.path(missing_classes, missing_classes)
classes_path <- c(classes_path, missing_paths)

## file names
file_name_list <- lapply(classes_path, function(x) 
  list.files(path = file.path(sample_dir,x), pattern = paste0("*.", sample_file_ext), full.names = TRUE, recursive = TRUE))

## sample names
sample_names_list <- lapply(file_name_list, function(x) tools::file_path_sans_ext(basename(x)))
names(sample_names_list) <- classes
sample_names <- unlist(sample_names_list)
sample_names_df <- purrr::map_df(sample_names_list, ~as.data.frame(.x), .id="id")
file_name_df <- purrr::map_df(file_name_list, ~as.data.frame(.x), .id="id")
sample_names_df <- cbind(substr(sample_names, 1, 4), sample_names_df, file_name_df[2])
colnames(sample_names_df) <- c("sample_id", "class", "sample_name", "file_name")
```

```{r csv data}
if (sample_file_ext == "csv") {
  
  process_csv <- function(file_name_list, fixed_objects) {
    cl <- makeCluster(detectCores() - 1) # Use one less than the number of available cores
    registerDoParallel(cl)
    
    result <- foreach(i = seq_along(file_name_list), .packages = c('dplyr', 'purrr')) %:% 
      foreach(j = seq_along(file_name_list[[i]])) %dopar% {
        
        ## extract m/z and intensity into list of lists
        spectrum <- tryCatch(read.csv(file_name_list[[i]][[j]], 
                                      col.names = c("mass", "intensity"), 
                                      skip = 8), 
                             error = function(e) tryCatch(read.csv(file_name_list[[i]][[j]], 
                                                                   col.names = c("mass", "intensity", "relative"), 
                                                                   skip = 8),
                                                          error = function(e) return(read.csv(file_name_list[[i]][[j]], 
                                                                                              col.names = c("mass", "intensity", "relative", "noise"), 
                                                                                              skip = 8))))
        
        ## Round m/z values to 3 decimal places
        spectrum$mass <- round(spectrum$mass, 3)
        
        ## FILTER #1: RETAIN PEAKS WITHIN MASS RANGE
        spectrum <- spectrum[spectrum$mass >= fixed_objects$mass_range[1] & spectrum$mass <= fixed_objects$mass_range[2], ]
        
        ## FILTER #2: RETAIN PEAKS WITH SNR >= SNR_thresh
        spectra_list <- if(length(spectrum) == 4) {
          ## Add SNR column
          spectrum$SNR <- (spectrum$intensity)/(spectrum$noise)
          
          spectrum <- tryCatch(subset(spectrum, SNR >= SNR_thresh),
                               error = function(e) return(spectrum))
          
          ## Remove columns "relative", "noise", and "SNR"
          spectrum <- subset(spectrum, select = -c(relative, noise, SNR))
          
          return(spectrum)
        }else if(length(spectrum) == 3){
          ## Remove column "relative"
          spectrum <- spectrum[, !colnames(spectrum) %in% c("relative")]
          
          return(spectrum)
        }else if(length(spectrum) == 2){
          return(spectrum)
        }
      }
    
    stopCluster(cl)
    return(result)
  }
  
  spectra_list <- process_csv(file_name_list, fixed_objects)
  
  ## Set sample names and classes of spectra_list
  spectra_list <- lapply(seq_along(spectra_list), \(i) setNames(spectra_list[[i]], sample_names_list[[i]]))
  names(spectra_list) <- classes
}
```

```{r excel data}
if (sample_file_ext == "xlsx") {
  
  process_xlsx <- function(file_name_list, fixed_objects) {
    cl <- makeCluster(detectCores() - 1) # Use one less than the number of available cores
    registerDoParallel(cl)
    
    result <- foreach(i = seq_along(file_name_list), .packages = c('dplyr', 'readxl', 'purrr')) %:% 
      foreach(j = seq_along(file_name_list[[i]])) %dopar% {
        
        ## extract m/z and intensity into list of lists
        spectrum <- tryCatch(read_excel(file_name_list[[i]][[j]], 
                                        col_names = c("mass", "intensity"), 
                                        skip = 8), 
                             error = function(e) tryCatch(read_excel(file_name_list[[i]][[j]], 
                                                                     col_names = c("mass", "intensity", "relative"), 
                                                                     skip = 8),
                                                          error = function(e) return(read_excel(file_name_list[[i]][[j]], 
                                                                                                col_names = c("mass", "intensity", "relative", "noise"), 
                                                                                                skip = 8))))
        
        ## Round m/z values to 3 decimal places
        spectrum$mass <- round(spectrum$mass, 3)
        
        ## FILTER #1: RETAIN PEAKS WITHIN MASS RANGE
        spectrum <- spectrum[spectrum$mass >= fixed_objects$mass_range[1] & spectrum$mass <= fixed_objects$mass_range[2], ]
        
        ## FILTER #2: RETAIN PEAKS WITH SNR >= SNR_thresh
        spectra_list <- if(length(spectrum) == 4) {
          ## Add SNR column
          spectrum$SNR <- (spectrum$intensity)/(spectrum$noise)
          
          spectrum <- tryCatch(subset(spectrum, SNR >= SNR_thresh),
                               error = function(e) return(spectrum))
          
          ## Remove columns "relative", "noise", and "SNR"
          spectrum <- subset(spectrum, select = -c(relative, noise, SNR))
          
          return(spectrum)
        }else if(length(spectrum) == 3){
          ## Remove column "relative"
          spectrum <- spectrum[, !colnames(spectrum) %in% c("relative")]
          
          return(spectrum)
        }else if(length(spectrum) == 2){
          return(spectrum)
        }
      }
    
    stopCluster(cl)
    return(result)
  }
  
  spectra_list <- process_xlsx(file_name_list, fixed_objects)
  
  ## Set sample names and classes of spectra_list
  spectra_list <- lapply(seq_along(spectra_list), \(i) setNames(spectra_list[[i]], sample_names_list[[i]]))
  names(spectra_list) <- classes
}
```

```{r raw Thermo data}
## If sample files are raw Thermo
if (sample_file_ext == "raw") {
  
  process_raw_thermo <- function(file_name_list, fixed_objects) {
    cl <- makeCluster(detectCores() - 1) # Use one less than the number of available cores
    registerDoParallel(cl)
    
    result <- foreach(i = seq_along(file_name_list), .packages = c('dplyr', 'rawrr', 'purrr')) %:% 
      foreach(j = seq_along(file_name_list[[i]])) %dopar% {
        
        ## Raw Thermo Data
        raw_data <- readSpectrum(file_name_list[[i]][[j]], scan = fixed_objects$scans)
        
        ## only keep relevant fields (mass, intensity, and noise) for each spectra
        ## round mass to 3 decimal places b/c mass accuracy of instrument is ~ 1 ppm or 4 b/c that's what Thermo reports?
        spectrum <- lapply(raw_data, function(x)
          tryCatch(data.frame(mass = round(x$centroid.mZ, 3),
                              intensity = x$centroid.intensity,
                              noise = x$noise,
                              SNR = ((x$centroid.intensity)/(x$noises))),
                   error = function(e) return(data.frame(mass = round(x$centroid.mZ, 3),
                                                         intensity = x$centroid.intensity,
                                                         noise = x$centroid.PreferredNoises,
                                                         SNR = ((x$centroid.intensity)/(x$centroid.PreferredNoises))
                   )
                   )
          )
        )
        
        ## FILTER #1: RETAIN PEAKS WITHIN MASS RANGE
        spectrum <- lapply(spectrum, function(x) x[x$mass >= fixed_objects$mass_range[1] & x$mass <= fixed_objects$mass_range[2], ])
        
        ## FILTER #2: RETAIN PEAKS WITH SNR >= SNR_thresh
        spectrum <- lapply(spectrum, function(x)
          tryCatch(subset(x, SNR >= SNR_thresh),
                   error = function(e) return(x)))
        
        ## FILTER #2b: RETAIN SCANS WITH >= 50 peaks
        spectrum <- Filter(function(z) nrow(z) >= 50, spectrum)
        
        ## Aggregate (sum) intensities of any peaks within scans that are duplicated due to rounding to 3 decimal places
        spectrum <- lapply(spectrum, function(x) aggregate(intensity ~ mass, data = x, FUN = sum))
        
        ## Merge scans into one spectrum per sample
        spectrum <- spectrum %>%
          reduce(full_join, by = "mass") %>%
          select(mass, matches("intensity"))
        
        ## Replace NA with 0
        spectrum <- replace(spectrum, is.na(spectrum), 0)
        
        ## Aggregate intensities of duplicate masses 
        ## AVERAGE INTENSITIES AS DONE IN THERMO FREESTYLE SOFTWARE (including zero values)
        spectrum <- data.frame(cbind(spectrum[, "mass"],
                                     tryCatch(rowMeans(spectrum[, !names(spectrum) %in% c("mass")]),
                                              error = function(e) return(spectrum[, "intensity"]))))
        
        ## Rename columns to mass and intensity
        colnames(spectrum) <- c("mass", "intensity")
        
        ## Order m/z smallest to largest
        spectrum <- spectrum[order(spectrum$mass), ]
      }
    
    stopCluster(cl)
    return(result)
  }
  
  spectra_list <- process_raw_thermo(file_name_list, fixed_objects)
  
  ## Set sample names and classes of spectra_list
  spectra_list <- lapply(seq_along(spectra_list), \(i) setNames(spectra_list[[i]], sample_names_list[[i]]))
  names(spectra_list) <- classes
}
```

```{r peak alignment}
if (peak_alignment_method == "clustering") {
  ## match peaks to centroids based on min and max of clusters
  ## fixed objects for exporting to parallel computing
  fixed_objects <- list(mass_range = mass_range, centroid_peaks = feature_peaks)
  
  centroid_peak_alignment <- function(spectra_list, fixed_objects) {
    cl <- makeCluster(detectCores() - 1) # Use one less than the number of available cores
    registerDoParallel(cl)
    
    result <- foreach(i = seq_along(spectra_list), .packages = c('dplyr', 'sqldf')) %:% 
      foreach(j = seq_along(spectra_list[[i]])) %dopar% {
        
        spectrum <- spectra_list[[i]][[j]]
        centroid_peaks <- fixed_objects$centroid_peaks
        
        feature_matched_spectrum <- sqldf("SELECT centroid_peaks.centroid, spectrum.*
          FROM spectrum,centroid_peaks
          WHERE spectrum.mass between centroid_peaks.min_mass AND centroid_peaks.max_mass")
        
        ## remove mass columns
        feature_matched_spectrum <- subset(feature_matched_spectrum, select = -c(mass))
        
        ## sum intensities of duplicates
        feature_matched_spectrum <- aggregate(intensity ~ ., data = feature_matched_spectrum, FUN = sum)
      }
    stopCluster(cl)
    return(result)
  }
  
  feature_matched_spectra <- centroid_peak_alignment(spectra_list, fixed_objects)
  feature_matched_spectra <- lapply(feature_matched_spectra, function(x) lapply(x, setNames, c("feature_mass", "intensity")))
  
} else if (peak_alignment_method == "binning") {
  
  ## Bin sample peaks
  preprocList <- lapply(spectra_list, function(x,z) get_data_matrix_binning(x,z), z=filtered_mz)
  
  aligned_spectra <- do.call(rbind, preprocList)
  
  colnames(aligned_spectra) <- filtered_mz
  rownames(aligned_spectra) <- sample_names
  
} else if (peak_alignment_method == "featurelist") {
  ## use filtered_mz as feature list
  feature_peaks <- filtered_mz
  
  ## Feature mz plus/minus instrument mass error, round to 3 decimal places
  mass_error <- unlist((ppm_error * feature_peaks)/1e6)
  
  feature_peaks <- data.frame(feature_mass = feature_peaks,
                              mass_error = mass_error,
                              mass_error_lower = round(feature_peaks - mass_error,3),
                              mass_error_upper = round(feature_peaks + mass_error,3))
  
  ## fixed objects for exporting to parallel computing
  fixed_objects <- list(mass_range = mass_range, feature_peaks = feature_peaks)
  
  feature_peak_alignment <- function(spectra_list, fixed_objects) {
    cl <- makeCluster(detectCores() - 1) # Use one less than the number of available cores
    registerDoParallel(cl)
    
    result <- foreach(i = seq_along(spectra_list), .packages = c('dplyr', 'sqldf')) %:% 
      foreach(j = seq_along(spectra_list[[i]])) %dopar% {
        
        spectrum <- spectra_list[[i]][[j]]
        feature_peaks <- fixed_objects$feature_peaks
        
        feature_matched_spectrum <- sqldf("SELECT feature_peaks.feature_mass, spectrum.*
          FROM spectrum,feature_peaks
          WHERE spectrum.mass between feature_peaks.mass_error_lower AND feature_peaks.mass_error_upper")
        
        ## remove mass columns
        feature_matched_spectrum <- subset(feature_matched_spectrum, select = -c(mass))
        
        ## sum intensities of duplicates
        feature_matched_spectrum <- aggregate(intensity ~ ., data = feature_matched_spectrum, FUN = sum)
      }
    stopCluster(cl)
    return(result)
  }
  
  feature_matched_spectra <- feature_peak_alignment(spectra_list, fixed_objects)
}

feature_matched_spectra <- feature_matched_spectra[lapply(feature_matched_spectra,length)>0]

## turn list of lists into dataframe
feature_matched_spectra <- lapply(feature_matched_spectra, function(x) x %>%
                                    reduce(full_join,by = "feature_mass")) %>% 
  reduce(full_join,by = "feature_mass")


## add on rows for filtered_mz peaks not matched to clinical samples
missing_filt_mz <- subset(filtered_mz, !(filtered_mz %in% feature_matched_spectra$feature_mass))
missing_df <- data.frame(matrix(ncol = ncol(feature_matched_spectra)-1, nrow = length(missing_filt_mz)))
missing_df <- cbind(missing_filt_mz, missing_df)
names(missing_df) <- names(feature_matched_spectra)

feature_matched_spectra <- rbind(feature_matched_spectra, missing_df)

## sort by target_mz
feature_matched_spectra <- feature_matched_spectra[order(feature_matched_spectra$feature_mass), ]

## feature_mass column to rownames
feature_matched_spectra <- feature_matched_spectra %>% 
  remove_rownames %>% 
  column_to_rownames(var = "feature_mass") %>% 
  as.data.frame()

## Add sample names as column names
colnames(feature_matched_spectra) <- sample_names

## Replace NA with 0
feature_matched_spectra <- replace(feature_matched_spectra, is.na(feature_matched_spectra), 0)

## Transpose so rows are samples and columns are masses
aligned_spectra <- t(feature_matched_spectra)
```

```{r normalization}
xall <- normalize_pixel(aligned_spectra, normalization_method)
```

```{r yall}
## create yall object 
yall <- factor(sample_names_df$class,levels=c(classes),labels = classes)
```

```{r all pairs}
## Extract trained features
# Extract coefficients at lambda.min
coef_sparse <- coef(model, s = cvmodel$lambda.min)

# If multinomial, coef_sparse is a list with one entry per class
if (is.list(coef_sparse)) {
  # For multinomial, you typically have multiple classes, take the first class for the rownames
  trained_features <- rownames(as.matrix(coef_sparse[[1]]))
} else {
  # For binomial or gaussian, directly convert to dense matrix
  trained_features <- rownames(as.matrix(coef_sparse))
}
trained_features <- trained_features[trained_features != "(Intercept)"]

## Below taken from Rob rc2 script for ratio calculations prior to Lasso
## Function to compute log ratios with bounds
compute_needed_ratios <- function(x, trained_features, min_valid = 10) {
  feature_splits <- strsplit(trained_features, "/")
  numerators <- sapply(feature_splits, `[`, 1)
  denominators <- sapply(feature_splits, `[`, 2)

  n_samples <- nrow(x)
  n_features <- length(trained_features)
  logratios <- matrix(NA_real_, nrow = n_samples, ncol = n_features)

  for (i in seq_along(trained_features)) {
    num <- numerators[i]
    denom <- denominators[i]

    if (num %in% colnames(x) && denom %in% colnames(x)) {
      ratio <- log(x[, num] / x[, denom])

      # Identify Inf/-Inf and calculate valid values
      neg_inf_mask <- is.infinite(ratio) & ratio < 0
      pos_inf_mask <- is.infinite(ratio) & ratio > 0
      ratio[neg_inf_mask | pos_inf_mask] <- NA

      valid_vals <- ratio[!is.na(ratio)]

      if (length(valid_vals) >= min_valid) {
        # Bound extremes
        q01 <- quantile(valid_vals, 0.01)
        q99 <- quantile(valid_vals, 0.99)
        
      } else {
        q01 <- quantile(valid_vals, 0.01, na.rm = TRUE)
        q99 <- quantile(valid_vals, 0.99, na.rm = TRUE)
      }
      
      ## Restore directionality of -Inf and +Inf
      ratio[neg_inf_mask] <- q01
      ratio[pos_inf_mask] <- q99
      
      logratios[, i] <- ratio
      
    } else {
      # Missing features
      logratios[, i] <- NA
    }
  }

  colnames(logratios) <- trained_features
  return(logratios)
}

## Perform log ratio lasso
xall1 <- compute_needed_ratios(x = xall, trained_features = trained_features)

## Remove any ratio columns that have only NA values
xall1[is.na(xall1)] <- 0

## Extract trained features
missing_features <- setdiff(trained_features, colnames(xall1))

## Add in missing features to match what was trained
xall1 <- as.data.frame(xall1)
if (length(missing_features) > 0) {
  for (x in missing_features) {
    xall1[[x]] <- 0
  }
}
xall1 <- xall1[, trained_features]
xall1 <- as.matrix(xall1)
```

```{r Testing}
test_p <- predict(model,
                  xall1,
                  s = cvmodel$lambda.min, ## Value of the penalty parameter lambda at which predictions are required
                  type = "response" ) ## to get prediction values rather than linker function values
```

```{r Testing Confusion Matrix}
# Drop the third dimension (lambda dimension) to convert to a 2D matrix
test_p_mat <- test_p[, , 1]

test_p_class <- apply(test_p_mat, 1, which.max)

test_p_class <- factor(test_p_class,levels=as.character(1:length(classes)),labels = classes)

test_cm <- table(True=yall,
                 Predict=test_p_class)
```

```{r Test Histogram}
test_df <- data.frame(file_name=sample_names, 
                      true_class=yall, 
                      prediction_class=test_p_class, 
                      probability=test_p_mat)

rownames(test_df) <- NULL

test_wrong_preds <- test_df %>%
  filter(true_class != prediction_class)
```

```{r save files, include = FALSE}
## test set files
write.csv(test_cm, 
          file = file.path(files_dir, "test_cm.csv"))

write.csv(test_df, 
          row.names = FALSE,
          file.path(files_dir, "test_preds.csv"))

write.csv(test_wrong_preds, 
          row.names = FALSE,
          file.path(files_dir, "misclassified_test_preds.csv"))
```

<br>

#### **Preprocessing and Statistical Model Settings**

```{r chunk3, fig.align = "center"}
if (peak_alignment_method == "clustering") {
  cluster_bin_size <- c("Cluster Height:", clust_h)
} else if (peak_alignment_method == "binning") {
  cluster_bin_size <- c("Bin Size:", "0.01")
} else if (peak_alignment_method == "featurelist") {
  cluster_bin_size <- c("Peak Mass Error:", paste0(ppm_error, " ppm"))
}

model_version <- sapply(str_split(model_path,"/"), tail, 1)

settings_df <- rbind(#c("SNR Threshold:", SNR_thresh),
                     c("Mass Range:", paste0('<i>m/z</i> ', mass_range[1], " - ", mass_range[2])),
                     c("Peak Alignment Method:", peak_alignment_method),
                     cluster_bin_size,
                     c("Normalization Method:", normalization_method),
                     c("Model Version:", model_version))

species <- gsub("_", " ", classes)

cols <- rev(c(hue_pal()(2)))

kable(settings_df,
      row.names = FALSE,
      align = "l",
      format = "html",
      escape = FALSE)%>%
  column_spec(1:2, width = "3in")%>% 
  kable_styling(full_width = F)
```

```{r chunk4}
test_recalls <- paste0(format(round(sapply(1:length(classes), function(x) test_cm[x,x]/sum(test_cm[x,])*100),2), nsmall = 2), "%")
test_counts <- sapply(1:length(classes), function(x) paste0(test_cm[x,x],"/",sum(test_cm[x,])))

test_accuracy <- cbind(paste0(species, ":"), test_recalls,test_counts)

test_accuracy <- rbind(test_accuracy, 
                       c("Overall Accuracy:", 
                         paste0(format(round(sum(diag(test_cm))/sum(test_cm)*100,2), nsmall = 2), "%"),
                         paste0(sum(diag(test_cm)),"/",sum(test_cm))))

colnames(test_accuracy) <- c("Species", "Recall Rate", "Correct Prediction/True Count")
```

<br>

#### **Test Set Confusion Matrix**

```{r fig.height=6, fig.width=6, message = FALSE, fig.align = "center"}
test_cm_df <- as.data.frame(test_cm)
test_cm_plot_df <- test_cm_df %>%
  mutate(plot.fill = case_when(
    True == Predict ~ "TP",
    True != Predict & Freq > 0 ~ "FP",
    True != Predict & Freq == 0 ~ "TN"))

ggplot(test_cm_plot_df, aes(Predict, True, label = Freq, fill = plot.fill)) + 
  geom_tile(color = "black") +
  geom_text() +
  scale_fill_manual(values = c("#f2e3c7", "white", "#c3fcb5"))+
  scale_x_discrete(position = "top") +
  scale_y_discrete(limits = rev) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0, vjust = 0), 
        plot.title.position = "plot",
        legend.position = "none",
        panel.grid = element_blank()) +
  #labs(title = "Test Set") +
  scale_y_discrete(labels=rev(species), limits=rev) +
  scale_x_discrete(labels=species, position = "top")
```

```{r, fig.align = "center"}
kable(test_accuracy,
      row.names = FALSE,
      align = "r") %>% 
  column_spec(1, bold = TRUE) %>%
  kable_minimal(full_width = FALSE,
                bootstrap_options = "condensed",
                html_font = "Calibri",
                font_size = 16)
```

<br>

#### **Test Set Misclassifications**

```{r chunk10}
## Add misclassified test samples
test_wrong_preds$true_class <- gsub("_", " ", test_wrong_preds$true_class)
test_wrong_preds$prediction_class <- gsub("_", " ", test_wrong_preds$prediction_class)
test_wrong_preds[, 4:ncol(test_wrong_preds)] <- round(test_wrong_preds[, 4:ncol(test_wrong_preds)], 3)
```

```{r chunk12, fig.align = "center"}
align <- paste0("lll",paste0(rep("c", ncol(test_wrong_preds)-3), collapse =""))

kable(test_wrong_preds,
      #caption = "Test Set Misclassifications",
      col.names = c("Sample Name", "True Class", "Predicted Class", species),
      row.names = FALSE,
      align = align)  %>%
  kable_styling(full_width = FALSE, 
                position = "center", 
                bootstrap_options = c("striped", "condensed"), 
                font_size = 12) %>%
  column_spec(1:3, width_min = "2in", width_max = "5in")  %>%
  add_header_above(c(" " = 3, "Probability" = ncol(test_wrong_preds)-3))
```

<br>

#### **Test Samples**

```{r chunk13}
## Add list of samples for training
## Add list of samples for testing

test_samples <- sort(test_df$file_name)

kable(test_samples,
      row.names = FALSE,
      col.names = c("Sample Names"),
      align = "l") %>%
  kable_minimal(full_width = FALSE,
                bootstrap_options = "condensed",
                html_font = "Calibri",
                font_size = 12)
```